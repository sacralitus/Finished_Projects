{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Основы NLP </center>\n",
    "<center><img src=\"https://miro.medium.com/max/2672/1*_Nb5AADlqVQJDa0YyNFKGA.jpeg\"></center>\n",
    "<p style=\"text-align: right;\"><em>Aвтор:</em> Ян Шинкевич</p>\n",
    "\n",
    "## <center> Часть 4. Машинное обучение. Сентимент-анализ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='black'> На данном занятии мы продолжим тему машинного обучения и попытаемся решить проблему сентимент-анализа (анализа тональности) алгоритмами как **классического машинного обучения**, так и **глубокого** (с использованием нейросетей).\n",
    "<br><br>В конце урока ты научишься строить:<br>\n",
    "<b>1. Модель сентимент-анализа с помощью Logistic Regression</b><br>\n",
    "<b>2. Модель сентимент-анализа с помощью LSTM нейросетей</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь вспомним наш <em>универсальный</em> рецепт для любой задачи Машинного Обучения: <br>\n",
    "<b>1. Датасет</b><br>\n",
    "<b>2. Предобработка данных</b><br>\n",
    "<b>3. Построение модели</b><br>\n",
    "<b>4. Обучение модели</b><br>\n",
    "<b>5. Оценка модели</b><br><br>\n",
    "\n",
    "Поехали!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>**Logistic Regression**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в прошлом занятии мы классифицировали смс-сообщения на предмет спама, то в данном эксперименте фокус нашего внимания падёт на твиты, которые распределены по 3-ём лэйблам: <b>positive</b>, <b>negative</b>, <b>neutral</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>1. Датасет</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь, загрузим наш датасет при помощи уже знакомой вам библиотеки <b>pandas</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "dataset = pandas.read_csv('polarized_twits.tsv.res', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просмотрим содержимое первых 5 сэмплов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что датасет немного грязноват и избыточен. Нас интересуют только колонки под номером 2 и 3.<br>\n",
    "Давайте избавимся от колонок 0 и 1 методом <b>drop</b> и для удобства переименуем колонки <em>2</em> и <em>3</em> в <em>label</em> и <em>text</em> соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=[0, 1], axis=1, inplace = True)\n",
    "dataset.rename({2:'label', 3:'twit'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просмотрите, как выглядит датасет самостоятельно (с помощью метода <em>head</em>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cделаем небольшой *EDA* (Exploratory Data Analysis) и посмотрим распределие лэйблов с помощью метода <b>value_counts</b>: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем\n",
    "dataset.label.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>2. Предобработка данных</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь, нам нужно нормализовать наши текстовые данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим функцию preprocess_data, с помощью которой посланное на вход предложение очищается от таких вещей, как стоп-слова, знаки препинания, цифры и т.д. Также функция будет производить стемминг над каждым словом:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = 'Theo Walcott is still shit :( watch Rafa and Johnny deal with him on Saturday...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer\n",
    "import re\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess_twit(twit):\n",
    "    \n",
    "    twit = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','', twit) #remove URLS\n",
    "    twit = re.sub('@[^\\s]+','', twit)  #remove Users\n",
    "    twit = re.sub('[:;^8=]{1}-?[)PD8o]+', 'positive_smile', twit) #smiles processing\n",
    "    twit = re.sub('[:;^8=]{1}-?[(/|]+', 'negative_smile', twit)\n",
    "    twit = twit.strip() # удаление пробелов по бокам \n",
    "    twit = twit.lower() # lower-case\n",
    "    twit = re.sub('[0-9]+', '', twit) # удаление цифр\n",
    "    twit = re.sub(r'\\b\\w\\b', '', twit) # удаление одноcимвольных токенов\n",
    "    twit = re.sub('[^A-Za-zА-Яа-я_\\s]+', '', twit) #удаление пунктуации\n",
    "    twit = [x for x in twit.split() if x not in stopwords.words('english')] # удаление стоп-слов\n",
    "    #twit = [stemmer.stem(x) for x in twit] # cтемминг\n",
    "\n",
    "    twit = ' '.join(twit) # cоединяем элементы списка\n",
    "    return twit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как эта функция сработает на нашем тестовом twit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(preprocess_twit(twit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы применить функцию к текстовому столбику нашего датасета (twit), воспользуемся методом pandas под названием **apply** и создадим колонку с очищенными твитами (refined_twit): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['refined_twit'] = dataset['twit'].apply(preprocess_twit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что у нас получилось. Выведите первые 10 сэмплов (через метод <em>head</em>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разбиение датасета**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь, отделим иксы от игреков. Затем разобьём все данные на <b>train</b> и <b>test</b> выборки в соотношении 0.7/0.3.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# отделяем лейблы от фич \n",
    "X = dataset.refined_twit\n",
    "y = dataset.label\n",
    "\n",
    "# разбиваем наш датасет на train и test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Векторизация данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в прошлом занятии мы обращались только к <b>Bag of Words</b> векторам, то в этом давайте попробуем использовать и <b>Word Embeddings</b>.<br><br>\n",
    "##### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не тратить много времени, Bag of Words векторизацию осуществим только с помощью бинарных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # импортируем векторизатор\n",
    "\n",
    "vectorizer_binary = CountVectorizer(binary=True) # указываем параметр \"binary\" = True (отсутствие/присутствие слова)\n",
    "vectorizer_binary.fit(X) # скармливаем нашему векторизатору все (!) данные\n",
    "X_binary_train = vectorizer_binary.transform(X_train) # векторизируем train partition \n",
    "X_binary_test = vectorizer_binary.transform(X_test) # векторизируем test partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ради интереса посмотрим, какой <b>размерности</b> наши вектора и <b>какие фичи</b> использует наш векторизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Матрица train выборки:\", X_binary_train.get_shape())\n",
    "print(\"Первый вектор:\\n\", X_binary_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Наши фичи:\", vectorizer_binary.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы уже знаем, за Word embeddings отвечает библиотека <b>gensim</b>, делающая процесс манипуляции над ними быстрым и удобным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеку \n",
    "import gensim.downloader as api\n",
    "\n",
    "# загружаем вектора GloVe\n",
    "glove_model = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def twit_vector(doc):\n",
    "    # Создаём вектор всего твита усреднением векторов всех слов в твите. Удаляем слово, если его нет в glove_model\"\"\"\n",
    "    doc = [word for word in doc.split() if word in glove_model.vocab]\n",
    "    return np.mean(glove_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_vector(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обернём в эмбеддинги все <b>train</b> и <b>test</b> твиты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_we = X_train.apply(twit_vector)\n",
    "X_test_we = X_test.apply(twit_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>3. Построение, обучение и оценка модели</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и планировали, используем <b>Логистическую регрессию</b> как пример алгоритма <em>классического машинного обучения</em>. <br>Cначала с <b>BoW векторами</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_model = LogisticRegression(solver = 'lbfgs', multi_class='auto')\n",
    "log_model.fit(X_binary_train, y_train)\n",
    "predictions = log_model.predict(X_binary_test)\n",
    "\n",
    "print(\"Logstic Regression with BoW accuracy score:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь накормим Логистическую регрессию нашими <b>Word embeddings</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(solver = 'lbfgs', multi_class='auto')\n",
    "log_model.fit(list(X_train_we), y_train)\n",
    "predictions = log_model.predict(list(X_test_we))\n",
    "\n",
    "print(\"Logstic Regression with Word Embeddings accuracy score:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <br><br><center> LSTM Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С классическим машинным обучением мы справились и увидели, на что оно способно. <br>\n",
    "Думаю, теперь нам стоить посмотреть одним глазком, какие результаты нам могут выдать нейросетевые архитектуры. А именно - <b>LSTM</b> (Long Short Term Memory), которая умеет запоминать <b>последовательности</b> токенов, а не только их наличие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нейросетях подход к обработке текста немного иной, в отличие от классического МО.<br>\n",
    "Поэтому здесь применим иную логику.<br>\n",
    "Для работы с нейросетями и их производными существует библиотека <b>keras</b>, с помощью которой творить нейросети легко и удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем Tokenizer (грубо говоря, аналог векторизатора)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# pad_sequences позволит уравнять все предложения по одной длине\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# библиотека для работы с математическими объектами\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим объект <em>Tokenizer</em> и поместим его в переменную <em>t</em>. Методом <b>fit_on_texts</b> поместим туда все наши твиты, находящиеся в переменной X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(X)\n",
    "# Посмотрим с помощью метода word_index, что представляет собой \"накормленный\" Tokenizer \n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем размер нашего словаря (количество уникальных слов), а потом преобразуем две выборки в численный формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размер нашего словаря (пригодится потом)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# Преобразование наших выборок в численный формат (где число = соответствующее ему слово)\n",
    "X_train_encoded = t.texts_to_sequences(X_train)\n",
    "X_test_encoded = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просмотрим, как теперь представлены наши документы (твиты):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*X_train_encoded[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравниваем все документы по самому длинному документу (переменная <b>max_length</b>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cамый длинный документ\n",
    "max_length = max([len(twit) for twit in X])\n",
    "\n",
    "# Пэдим каждую выборку\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут мы создадим <b>матрицу эмбдеддингов</b>, которая будет нужна для того, чтобы при поступлении в нейронную сеть наши токены оборачивались в соответствующий им эмбеддинг, а потом в этом же виде проходили дальше по слоям нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# матрица эмбеддингов размера vocab_size X 50 (размер наших glove векторов), заполненная нулями\n",
    "embedding_matrix = numpy.zeros((vocab_size, 50))\n",
    "\n",
    "# заполнение матрицы реальными векторами\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = glove_model[word] if word in glove_model.vocab else None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, переведём наши лэйблы в дискретные величины (этого требует нейросеть):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.replace({'positive':0,'neutral':1,'negative':2})\n",
    "y_test = y_test.replace({'positive':0,'neutral':1,'negative':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Построение нейросетевой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding,LSTM\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Input / Embdedding\n",
    "model.add(Embedding(vocab_size, output_dim = 50, \n",
    "                    weights=[embedding_matrix], \n",
    "                    mask_zero=True, \n",
    "                    trainable=True))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение нейросети на 2 эпохах и получение конечного <b>accuracy</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# кладём X и y в нашу нейросеть\n",
    "model.fit(X_train_padded, y_train, epochs=2, batch_size=64)\n",
    "\n",
    "# смотрим качество предсказаний, сделанных нейросетью на тестовой выборке\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "\n",
    "# наша accuracy\n",
    "print('\\nAccuracy: {}'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "<b>Вот такие, и огромное множество других задач обработки естественного языка (и не только) решает Machine Learning. <br>Сегодня ей под силу действительно целая масса реальных проблем разного типа, содержания и важности. Можно сказать, что современный мир постепенно \"подсаживается\" на данную область, о чём, вы, постоянно находясь в интернет-пространстве, наверное, и сами знаете :)<br><br>\n",
    "<center>Круто, что не поленились пройти этот курс вместе с его авторами! Эти самые авторы искренне хотят, чтобы в нашем университете понимание Обработки Ествественного Языка сдвинулось с давно уже устарешвего уровня на более-менее современный и интересный!<br> Однако не стоит забывать, что мы попытались дать сильный и первоначальный толчок, открыв только ОСНОВЫ(!), с расчетом на то, что углубление подобными вещами происходить самостоятельно, на началах вашего собственного интереса :) </center><br><br>\n",
    "<center>Спасибо за внимание!</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
